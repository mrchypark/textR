<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>텍스트 분석을 위한 R</title>
    <meta charset="utf-8" />
    <meta name="author" content="박찬엽" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-47822682-14', 'auto');
    ga('send', 'pageview');
    </script>
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: center, middle, title-slide, 

## 텍스트 분석을 위한 R 

### &lt;https://mrchypark.github.io/textR&gt;

#### [[pdf버전]](https://github.com/mrchypark/textR/blob/master/docs/textR.pdf) [[문의하기]](http://pf.kakao.com/_RXANd) [[의견 및 오류 신고]](https://github.com/mrchypark/textR/issues/new)
#### [스타누르기](https://github.com/mrchypark/textR)는 컨텐츠 제작자를 춤추게 합니다.

### 박찬엽

### 2018년 09월 18일.small[(update: 2019년 4월 17일)]
---
class: center, middle, title-slide, 

## 텍스트 관련 R 패키지 설치 가이드

### &lt;https://mrchypark.github.io/textR/installation&gt;    

### [pdf 다운로드](https://github.com/mrchypark/textR/blob/master/docs/installation.pdf)

.footnote[\* R 3.5.1 을 기준으로 작성하였습니다.]

---
class: center, middle, title-slide, 

## 사전 지식 1
## 함수를 연결하는 파이프 연산자(%&gt;%)

![](https://raw.githubusercontent.com/mrchypark/dabrp_classnote3/master/docs/img/pipes.png)

---

class: 
## 파이프 연산자(%&gt;%)

함수를 중첩해서 사용할 일이 점점 빈번해 짐

```{}
plot(diff(log(sample(rnorm(10000,mean=10,sd=1),size=100,replace=FALSE))),col="red",type="l")
```

--
class: 
### %&gt;%를 사용하면 

.pull-left[

1. 생각의 순서대로 함수를 작성할 수 있음
1. 중간 변수 저장을 할 필요가 없음
1. 순서가 읽이 용이하여 기억하기 좋음
]
.pull-right[
```{}
rnorm(10000,mean=10,sd=1) %&gt;%
  sample(size=100,replace=FALSE) %&gt;%
  log %&gt;%
  diff %&gt;%
  plot(col="red",type="l")
```
]
---
class: 

## 파이프 연산자(%&gt;%)

flights 데이터에 파이프 연산자 사용예 1


```r
flights %&gt;%
  group_by(year,month,day) %&gt;%
  summarise(delay=mean(dep_delay, na.rm = TRUE))
```

```
## # A tibble: 365 x 4
## # Groups:   year, month [?]
##     year month   day delay
##    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
##  1  2013     1     1 11.5 
##  2  2013     1     2 13.9 
##  3  2013     1     3 11.0 
##  4  2013     1     4  8.95
##  5  2013     1     5  5.73
##  6  2013     1     6  7.15
##  7  2013     1     7  5.42
##  8  2013     1     8  2.55
##  9  2013     1     9  2.28
## 10  2013     1    10  2.84
## # ... with 355 more rows
```
---
class: 
## 파이프 연산자(%&gt;%)

group_by()는 filter()와도 함께 사용할 수 있음


```r
popular_dests &lt;- flights %&gt;% 
  group_by(dest) %&gt;% 
  filter(n() &gt; 365)
popular_dests
```

```
## # A tibble: 332,577 x 19
## # Groups:   dest [77]
##     year month   day dep_time sched_dep_time dep_delay arr_time
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;
##  1  2013     1     1      517            515         2      830
##  2  2013     1     1      533            529         4      850
##  3  2013     1     1      542            540         2      923
##  4  2013     1     1      544            545        -1     1004
##  5  2013     1     1      554            600        -6      812
##  6  2013     1     1      554            558        -4      740
##  7  2013     1     1      555            600        -5      913
##  8  2013     1     1      557            600        -3      709
##  9  2013     1     1      557            600        -3      838
## 10  2013     1     1      558            600        -2      753
## # ... with 332,567 more rows, and 12 more variables: sched_arr_time &lt;int&gt;,
## #   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,
## #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,
## #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;
```

---
class: 
## 파이프 연산자(%&gt;%)

사용할 데이터부터 순서대로 함수를 작성할 수 있는 장점


```r
popular_dests %&gt;% 
  filter(arr_delay &gt; 0) %&gt;% 
  mutate(prop_delay = arr_delay / sum(arr_delay)) %&gt;% 
  select(year:day, dest, arr_delay, prop_delay)
```

```
## # A tibble: 131,106 x 6
## # Groups:   dest [77]
##     year month   day dest  arr_delay prop_delay
##    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;
##  1  2013     1     1 IAH          11  0.000111 
##  2  2013     1     1 IAH          20  0.000201 
##  3  2013     1     1 MIA          33  0.000235 
##  4  2013     1     1 ORD          12  0.0000424
##  5  2013     1     1 FLL          19  0.0000938
##  6  2013     1     1 ORD           8  0.0000283
##  7  2013     1     1 LAX           7  0.0000344
##  8  2013     1     1 DFW          31  0.000282 
##  9  2013     1     1 ATL          12  0.0000400
## 10  2013     1     1 DTW          16  0.000116 
## # ... with 131,096 more rows
```

---

class: center, middle, title-slide, 

## 사전 지식 2
# tidy data + universe 

[![](https://github.com/tidyverse/tidyverse/raw/master/man/figures/logo.png)][1]

---
class: 
## [tidyverse][1] 패키지는

.pull-left[
1. RStudio가 개발, 관리하는 패키지    
1. 공식 문서가 매우 잘 되어 있음    
1. 사용자층이 두터워 영어로 검색하면 많은 질답을 찾을 수 있음    
1. 커뮤니티 설명글도 매우 많음    
1. 6개의 핵심 패키지 포함 23가지 패키지로 이루어진 메타 패키지    
1. tidy data 라는 사상과 파이프 연산자로 대동단결    
1. 사상에 영감을 받아 맞춰서 제작하는 개인 패키지가 많음(ex&gt; [tidyquant](https://github.com/business-science/tidyquant), [tidytext](https://github.com/juliasilge/tidytext) 등)
]
.pull-right[

```r
if (!requireNamespace("tidyverse")){
  install.packages("tidyverse")}
library(tidyverse)
```
]
---
class: 
## tidy data 란

1. [Hadley Wickham](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html)  2. [고감자님의 블로그](http://freesearch.pe.kr/archives/3942)  3. [헬로우데이터과학](http://www.hellodatascience.com/?p=287)


1.1 Each variable forms a column.    
1.2 각 변수는 개별의 열(column)으로 존재한다.    
1.3 각 열에는 개별 속성이 들어간다.    

2.1 Each observation forms a row.    
2.2 각 관측치는 행(row)를 구성한다.    
2.3 각 행에는 개별 관찰 항목이 들어간다.    

3.1 Each type of observational unit forms a table.    
3.2 각 테이블은 단 하나의 관측기준에 의해서 조직된 데이터를 저장한다.    
3.3 각 테이블에는 단일 유형의 데이터가 들어간다.    

.footnote[
  \* 출처 : [금융데이터 분석을 위한 R 입문][2]
]
---
class: 
## tidy data 란

![](http://garrettgman.github.io/images/tidy-1.png)

.footnote[
  \* 출처 : [Garrett Grolemund의 Data Science with R 블로그](http://garrettgman.github.io/tidying/)
]

---
class: 
## long form vs wide form

.pull-left[
### long form

1. 컴퓨터가 계산하기 좋은 모양
1. tidy data의 요건을 충족
1. tidyverse의 패키지 대부분의 입력 형태
]
.pull-right[
### wide form

1. 사람이 눈으로 보기 좋은 모양
1. 2개 변수에 대한 값만 확인 가능
1. dashboard 형이라고도 하며 조인 등 연산이 어려움
]

---

class: 

## tidy .blue[text] data 란

* a table with one-token-per-row    
* 한 행(row)에 한 토큰(token)으로 테이블을 구성해야 한다.

--

## 그럼 Token 이란?

글자 중 의미를 가진 단위를 총칭.    
tokenization은 가지고 있는 텍스트 자원을 token 단위로 나누는 것을 뜻함.     
ex&gt; 자소(자음, 모음), 음소(글자), 형태소, 단어, n-gram 등

---
class: 

## [tidytext][tidytext] 패키지 소개

.pull-left[
* 한 행(row)에 한 토큰(token)으로 테이블을 구성하기 위한 패키지    
* 파이프 연산자를 지원    
* 여러 가지 token과 tm 패키지와의 호환 기능을 제공
* 자세히 소개하는 [온라인 사이트(영문)][tidytextmining]

  
  ```r
  if(!requireNamespace("tidytext")){
    install.packages("tidytext")
  }
  library(tidytext)
  ```

]
.pull-right[
  .center[
    [&lt;img src=https://www.tidytextmining.com/images/cover.png width=60%&gt;](http://amzn.to/2tZkmxG)
  ]
]

---
class: 

## 데이터 패키지 소개

### [presidentSpeechKr](https://forkonlp.github.io/presidentSpeechKr/)

대통령 기록 연구실의 대통령 연설문을 제공


```r
if(!requireNamespace("presidentSpeechKr")){
  remotes::install_github("forkonlp/presidentSpeechKr")
}
library(presidentSpeechKr)
```

---
class: 

## 대통령 조건 확인

```r
get_president()
```

```
##  [1] "이승만" "윤보선" "박정희" "최규하" "전두환" "노태우" "김영삼"
##  [8] "김대중" "노무현" "이명박"
```
--
class: 

## 연설 분야 조건 확인

```r
get_field()
```

```
##  [1] "국정전반"       "정치/사회"      "산업/경제"      "외교/통상"     
##  [5] "국방"           "과학기술정보"   "교육"           "문화/체육/관광"
##  [9] "환경"           "기타"
```
---
class: 

## 연설 유형 확인

```r
get_event()
```

```
##  [1] "취임사"      "신년사"      "국회연설"    "기념사"      "만찬사"     
##  [6] "환영사"      "치사"        "성명/담화문" "라디오연설"  "기타"
```

--
class: 

## 연설 리스트 데이터


```r
data(spidx)
str(spidx)
```

```
## Classes 'tbl_df', 'tbl' and 'data.frame':	6681 obs. of  6 variables:
##  $ president: chr  "이승만" "이승만" "이승만" "이승만" ...
##  $ field    : chr  "기타" "국정전반" "정치/사회" "국정전반" ...
##  $ event    : chr  "성명/담화문" "취임사" "성명/담화문" "기타" ...
##  $ title    : chr  "학생제군에게" "대통령 취임사(大統領就任辭) " "민족이 원하는 길을 따를 결심, 국무총리 인준 부결에 대하여 " "미급점(未及點) 육성하라 " ...
##  $ date     : chr  "1948" "1948.07.24" "1948.07.29" "1948.08.09" ...
##  $ link     : chr  "http://www.pa.go.kr/research/contents/speech/index.jsp?spMode=view&amp;catid=c_pa02062&amp;artid=1310475" "http://www.pa.go.kr/research/contents/speech/index.jsp?spMode=view&amp;catid=c_pa02062&amp;artid=1310440" "http://www.pa.go.kr/research/contents/speech/index.jsp?spMode=view&amp;catid=c_pa02062&amp;artid=1310441" "http://www.pa.go.kr/research/contents/speech/index.jsp?spMode=view&amp;catid=c_pa02062&amp;artid=1310442" ...
```

---

## 대통령 조건 연설 검색


```r
library(dplyr)
spidx %&gt;% 
  filter(president == "윤보선")
```

```
## # A tibble: 3 x 6
##   president field  event  title            date   link                    
##   &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;  &lt;chr&gt;                   
## 1 윤보선    국정전반~ 취임사 제2대 윤보선 대통령 취임사~ 1960.~ http://www.pa.go.kr/res~
## 2 윤보선    기타   기타   "윤보선 대통령 부산연설 \~ 1960.~ http://www.pa.go.kr/res~
## 3 윤보선    기타   기타   "윤보선 대통령 대구연설 \~ 1960.~ http://www.pa.go.kr/res~
```

---
class: 

## 연설문 텍스트 가져오기

```r
tar &lt;- 
  spidx %&gt;% 
  filter(president == "윤보선") %&gt;% 
  select(link) %&gt;% 
  top_n(1)
```

```
## Selecting by link
```

```r
get_speech(tar)
```

```
## # A tibble: 1 x 9
##   title    date   president place field event source paragraph content    
##   &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;      
## 1 "윤보선 대통~ 1960.~ 윤보선    지역  기타  기타  ""             1 "영남지방의 한해 ~
```

---

## 연습문제

1. presidentSpeechKr 패키지에서 검색할 수 있는 대통령은 총 몇명인가요?

1. **윤보선** 대통령과 **박정희** 대통령은 각각 몇 개의 연설문이 있나요?

1. `nchar()` 함수는 글자수를 세주는 함수입니다. **최규하** 대통령의 취임사는 총 몇 글자 인가요?

---
class: center, middle, title-slide, 

## 단어 단위로 잘라보자!

---
class: 

## `unnest_tokens()` 함수

기본값인 단어 단위(특수문자 제거, 띄어쓰기 기준) token으로 동작.


```r
get_speech(tar) %&gt;% 
  select(president, content) %&gt;% 
  unnest_tokens(word, content)
```

```
## # A tibble: 100 x 2
##    president word        
##    &lt;chr&gt;     &lt;chr&gt;       
##  1 윤보선    영남지방의  
##  2 윤보선    한해        
##  3 윤보선    상황을      
##  4 윤보선    시찰차      
##  5 윤보선    십오일      
##  6 윤보선    상오        
##  7 윤보선    구시        
##  8 윤보선    삼십분      
##  9 윤보선    특별기편으로
## 10 윤보선    대구에      
## # ... with 90 more rows
```

---
class: 

## `unnest_tokens()` 함수 설명

텍스트 데이터를 token 단위로 풀어내는 함수

```{}
unnest_tokens(
  tbl = 텍스트 데이터,      # 다루고자 하는 텍스트 데이터 객체
  output = 결과열의 이름,   # token화의 결과가 작성될 열의 이름
  input = 목표 텍스트 열,   # 텍스트 데이터 객체 내의 텍스트 열
  token = "word",         # 기본값(띄어쓰기 단위)이 있어 생략 가능
  ...                     # 기타 옵션들
  )
```

---
class: 


```r
# 연설문 중 1개를 가져와서
get_speech(tar) %&gt;%
  # 대통령 컬럼과 연설문 컬럼만 선택한 후
  select(president, content) %&gt;% 
  # 연설문 컬럼을 word 단위로 쪼갠 결과물을 word라는 컬럼으로 출력
  unnest_tokens(word, content)
```

```
## # A tibble: 100 x 2
##    president word        
##    &lt;chr&gt;     &lt;chr&gt;       
##  1 윤보선    영남지방의  
##  2 윤보선    한해        
##  3 윤보선    상황을      
##  4 윤보선    시찰차      
##  5 윤보선    십오일      
##  6 윤보선    상오        
##  7 윤보선    구시        
##  8 윤보선    삼십분      
##  9 윤보선    특별기편으로
## 10 윤보선    대구에      
## # ... with 90 more rows
```

---

## 띄어쓰기 단위로 나눴을 때 문제점

.pull-left[
  .blue[**하다**]가 몇 가지 단어가 되는지    
  &lt;https://namu.wiki/w/%ED%8C%8C%EC%9D%BC:M4nNWBR.png&gt;
]

.pull-right[

  &lt;img src=https://user-images.githubusercontent.com/6179259/45862373-5d2c2980-bdac-11e8-9247-3ebde14583e8.png width=60%&gt;
]

---
class: 

## 한글의 특징 .yellow[형태소]

형태소란 : 의미를 가지는 최소 단위

&gt; 철수가 밥을 먹었다.


```
## $철수가
##  [1] "철수/ncpa+가/jcc"          "철수/ncpa+가/jcs"         
##  [3] "철수/ncpa+가/ncn"          "철수/ncpa+이/jp+가/ecc"   
##  [5] "철수/ncn+가/jcc"           "철수/ncn+가/jcs"          
##  [7] "철수/ncn+가/ncn"           "철수/ncn+이/jp+가/ecc"    
##  [9] "철/xp+수가/ncn"            "철/xp+수/ncn+가/jcc"      
## [11] "철/xp+수/ncn+가/jcs"       "철/xp+수/ncn+이/jp+가/ecc"
## 
## $밥을
## [1] "밥/ncn+을/jco"  "밥/ncn+을/jcs"  "밥/ncpa+을/jco" "밥/ncpa+을/jcs"
## [5] "밥/ncps+을/jco" "밥/ncps+을/jcs"
## 
## $먹었다
## [1] "먹/pvg+었/ep+다/ef"
## 
## $.
## [1] "./sf" "./sy"
```

---
class: 

.pull-left[![](https://github.com/haven-jeon/KoNLP/blob/master/etcs/figures/konlp_tags.png?raw=true)]
.pull-right[
  ### [크게 보기](https://github.com/haven-jeon/KoNLP/blob/master/etcs/figures/konlp_tags.png?raw=true)
  ### [여러 체계의 형태소 품사](https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0)
]

---
class: 

## R의 대표적인 형태소 분석기

.pull-left[
  ### RcppMeCab
  1. 일본어 형태소 분석기인 mecab 기반
  1. C++ 로 작성하여 속도가 매우 빠름
  1. 일본어, 중국어 등도 사용 가능
  1. 형태소 분석 함수를 제공
  1. 띄어쓰기에 덜 민감함
]
.pull-right[
  ### KoNLP
  1. 가장 유명한 형태소 분석기
  1. java로 작성된 한나눔 분석기 기반
  1. 우리샘, NiaDIC 등 자체 사전
  1. 텍스트 분석을 위한 기능들을 제공
  1. 친절한 [설명서](https://github.com/haven-jeon/KoNLP/blob/master/etcs/KoNLP-API.md)
]

---
class: center, middle, title-slide, 

## RcppMeCab 설치 확인

---
class: 

## RcppMeCab 실행


```r
library(RcppMeCab)
pos("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
```

```
## $`&lt;U+00B7&gt;&lt;U+0535&gt;&lt;U+00A5&gt;&lt;U+00B8&gt;&lt;U+00B6&gt;&lt;U+01AE&gt;&lt;U+00B0&gt;&lt;U+00A1&gt; &lt;c6&gt;&lt;U+01F8&gt;&lt;c5&gt;&lt;c7&gt;&lt;U+03F0&gt;&lt;ed&gt; &lt;c0&gt;&lt;U+05B4&gt;&lt;c2&gt; &lt;c8&gt;渶&lt;U+00B4&gt;&lt;c3&gt; &lt;U+00BE&gt;&lt;e7&gt;&lt;U+00B3&gt;&lt;e4&gt; &lt;U+0121&gt;&lt;U+0172&gt;&lt;c0&gt;&lt;cc&gt; &lt;U+00B3&gt;&lt;ed&gt;&lt;U+00B6&gt;&lt;f5&gt;&lt;c0&gt;&lt;cc&gt; &lt;U+00B5&gt;&lt;U+01F0&gt;&lt;ed&gt; &lt;c0&gt;&lt;U+05B4&gt;&lt;d9&gt;.`
##  [1] "&lt;U+00B7&gt;&lt;U+0535&gt;&lt;U+00A5&gt;&lt;U+00B8&gt;&lt;U+00B6&gt;/SY" "&lt;U+01AE&gt;/SL"            
##  [3] "&lt;U+00B0&gt;&lt;U+00A1&gt;/SY"     "&lt;c6&gt;&lt;c7&gt;/SL"            
##  [5] "&lt;U+00B8&gt;/SY"             "&lt;c5&gt;&lt;c7&gt;/SL"            
##  [7] "&lt;U+03F0&gt;/SL"             "&lt;ed&gt; &lt;c0&gt;&lt;U+05B4&gt;&lt;c2&gt; /SY"
##  [9] "&lt;c8&gt;&lt;e6&gt;/SL"             "&lt;U+00B8&gt;&lt;U+00B6&gt;&lt;U+00B4&gt;/SY"
## [11] "&lt;c3&gt; /SL"                "&lt;U+00BE&gt;/SY"            
## [13] "&lt;e7&gt;&lt;U+00B3&gt;&lt;e4&gt;/SH"     "&lt;U+0121&gt;&lt;U+0172&gt;/SL"    
## [15] "&lt;c0&gt;&lt;cc&gt;/SY"             "&lt;U+00B3&gt;&lt;ed&gt;&lt;U+00B6&gt;&lt;f5&gt;&lt;c0&gt;&lt;cc&gt;/SY"
## [17] "&lt;U+00B5&gt;/SY"             "&lt;U+01F0&gt;/SL"            
## [19] "&lt;ed&gt; &lt;c0&gt;&lt;U+05B4&gt;&lt;d9&gt;./SY"
```

---
class: center, middle, title-slide, 

## KoNLP 설치 확인

---
class: 

## KoNLP 실행


```r
library(KoNLP)
SimplePos09("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
```

```
## $롯데마트가
## [1] "롯데마트/N+가/J"
## 
## $판매하고
## [1] "판매/N+하고/J"
## 
## $있는
## [1] "있/P+는/E"
## 
## $흑마늘
## [1] "흑마늘/N"
## 
## $양념
## [1] "양념/N"
## 
## $치킨이
## [1] "치킨/N+이/J"
## 
## $논란이
## [1] "논란/N+이/J"
## 
## $되고
## [1] "되/P+고/E"
## 
## $있다
## [1] "있/P+다/E"
## 
## $.
## [1] "./S"
```

---

## 연습문제

1. **김영삼** 대통령의 첫 국무회의 연설문을 띄어쓰기 단위로 자르면 총 몇 단어인가요?

1. **노태우** 대통령의 취임사를 `RcppMeCab` 패키지로 형태소 분석한 결과를 출력하세요.

1. **김대중** 대통령의 취임사를 `KoNLP` 패키지의 `SimplePos09()` 함수로 형태소 분석한 결과를 출력하세요.

---
class: title, center, middle
## 복습

---

## 준비


```r
library(tidytext)
library(dplyr)
library(presidentSpeech)

tar &lt;- 
  spidx %&gt;% 
  filter(president == "이명박") %&gt;% 
  arrange(desc(date)) %&gt;% 
  pull(link) %&gt;% 
  .[1]
```

---
class: 

## `unnest_tokens()` 함수 설명

텍스트 데이터를 token 단위로 풀어내는 함수

```{}
unnest_tokens(
  tbl = 텍스트 데이터,      # 다루고자 하는 텍스트 데이터 객체
  output = 결과열의 이름,   # token화의 결과가 작성될 열의 이름
  input = 목표 텍스트 열,   # 텍스트 데이터 객체 내의 텍스트 열
  token = "word",           # 기본값(띄어쓰기 단위)이 있어 생략 가능
  ...                       # 기타 옵션들
  )
```

---

## 연설문 가져와서 word(띄어쓰기) 단위로 나누기


```r
# 연설문 중 1개를 가져와서
get_speech(tar, paragraph = T) %&gt;%
  # 문단 컬럼과 연설문 컬럼만 선택한 후
  select(paragraph, content) %&gt;% 
  # 연설문 컬럼을 word 단위로 쪼갠 결과물을 word라는 컬럼으로 출력
  unnest_tokens(word, content)
```

```
## # A tibble: 708 x 2
##    paragraph word        
##        &lt;int&gt; &lt;chr&gt;       
##  1         1 안녕하십니까
##  2         1 대통령입니다
##  3         2 2           
##  4         2 년          
##  5         2 전          
##  6         2 오늘        
##  7         2 1           
##  8         2 월          
##  9         2 21          
## 10         2 일은        
## # ... with 698 more rows
```

---
class: 

## R의 대표적인 형태소 분석기

.pull-left[
  ### RcppMeCab
  1. 일본어 형태소 분석기인 mecab 기반
  1. C++ 로 작성하여 속도가 매우 빠름
  1. 일본어, 중국어 등도 사용 가능
  1. 형태소 분석 함수를 제공
  1. 띄어쓰기에 덜 민감함
]
.pull-right[
  ### KoNLP
  1. 가장 유명한 형태소 분석기
  1. java로 작성된 한나눔 분석기 기반
  1. 우리샘, NiaDIC 등 자체 사전
  1. 텍스트 분석을 위한 기능들을 제공
  1. 친절한 [설명서](https://github.com/haven-jeon/KoNLP/blob/master/etcs/KoNLP-API.md)
]

---

## 형태소 분석기


```r
library(KoNLP)
SimplePos09("한글 텍스트를 분석하기 위해서는 형태소 분석기가 필수다.")
```

```
## $한글
## [1] "한글/N"
## 
## $텍스트를
## [1] "텍스트/N+를/J"
## 
## $분석하기
## [1] "분석하기/N"
## 
## $위해서는
## [1] "위하/P+어서/E+는/J"
## 
## $형태소
## [1] "형태소/N"
## 
## $분석기가
## [1] "분석기/N+가/J"
## 
## $필수다
## [1] "피/P+ㄹ/E+수/N+이/J+다/E"
## 
## $.
## [1] "./S"
```

---
class: title, center, middle
## 시작

---

## 형태소 분석기와 함께 사용하기

```{}
unnest_tokens(
  tbl = 텍스트 데이터,      
  output = 결과열의 이름,   
  input = 목표 텍스트 열,   
  token = "word",         &lt;---- 여기에 형태소 분석 함수를 적용
  ...                     
  )
```

---

## tidy 하게 형태소 분석하기

KoNLP의 `SimplePos09()` 함수를 활용해서 형태소 단위로 쪼갠 데이터를 만듭니다.

.pull-left[
```r
library(KoNLP)
pos_res &lt;- 
# 연설문 중 1개를 가져와서
  get_speech(tar, paragraph = T) %&gt;%
  # 문단 컬럼과 연설문 컬럼만 선택한 후
  select(paragraph, content) %&gt;% 
  # 연설문 컬럼을 형태소 단위로 쪼개 
  # pos라는 컬럼으로 출력
  unnest_tokens(pos, content, 
                token = SimplePos09) %&gt;% 
  # pos 결과물의 순서 보장을 위해 순서 값을 추가
  mutate(pos_order = 1:n())
pos_res
```
]
.pull-right[

```
## # A tibble: 10,969 x 3
##    paragraph pos                   pos_order
##        &lt;int&gt; &lt;chr&gt;                     &lt;int&gt;
##  1         1 사회자(sbs/n                  1
##  2         1 논설위원/n                    2
##  3         1 김형민/n+)/s                  3
##  4         1 여러분/n                      4
##  5         1 안녕하십니까/n+,/s            5
##  6         1 ‘대통령과/n+의/j              6
##  7         1 원탁/n                        7
##  8         1 대화/n                        8
##  9         1 어떻/p+게/e                   9
## 10         1 생각하십니까,’/n+의/j        10
## # ... with 10,959 more rows
```
]

---

.pull-left[![](https://github.com/haven-jeon/KoNLP/blob/master/etcs/figures/konlp_tags.png?raw=true)]
.pull-right[
  ### [크게 보기](https://github.com/haven-jeon/KoNLP/blob/master/etcs/figures/konlp_tags.png?raw=true)
  ### [여러 체계의 형태소 품사](https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0)
]

---

## 형태소 일부만 가져오기

신뢰할 수 있는 stop word 사전 등이 없기 때문에, 형태소 분석 후 필요한 형태소만 활용

.pull-left[
```r
pos_res %&gt;%
  # 우선 `filter()` 와 `grepl()` 함수
  # 활용하여 명사만 추출
  filter(grepl("/n", pos)) %&gt;% 
  # 형태소 정보를 제거
  mutate(pos_done = gsub("/.*$", "", pos)) -&gt; 
  n_done
n_done
```
]
.pull-right[

```
## # A tibble: 5,939 x 4
##    paragraph pos                   pos_order pos_done      
##        &lt;int&gt; &lt;chr&gt;                     &lt;int&gt; &lt;chr&gt;         
##  1         1 사회자(sbs/n                  1 사회자(sbs    
##  2         1 논설위원/n                    2 논설위원      
##  3         1 김형민/n+)/s                  3 김형민        
##  4         1 여러분/n                      4 여러분        
##  5         1 안녕하십니까/n+,/s            5 안녕하십니까  
##  6         1 ‘대통령과/n+의/j              6 ‘대통령과     
##  7         1 원탁/n                        7 원탁          
##  8         1 대화/n                        8 대화          
##  9         1 생각하십니까,’/n+의/j        10 생각하십니까,’
## 10         1 사회/n+를/j                  11 사회          
## # ... with 5,929 more rows
```
]

---

## 명사, 형용사, 동사 가져오기

형태소 분석 후 한 글자는 전후 맥락 없이 의미를 파악하기 어렵기 때문에 제거

.pull-left[
```r
vv_done &lt;- 
  pos_res %&gt;%
  filter(grepl("/vv", pos)) %&gt;% 
  mutate(pos_done = gsub("/.*$", "다", pos))

va_done &lt;- 
  pos_res %&gt;%
  filter(grepl("/va", pos)) %&gt;% 
  mutate(pos_done = gsub("/.*$", "다", pos))

pos_done &lt;- 
  bind_rows(n_done, vv_done, va_done) %&gt;% 
  arrange(pos_order) %&gt;% 
  # nchar() 함수는 글자수를 출력
  filter(nchar(pos_done)&gt;1) %&gt;% 
  select(paragraph, pos_done)
pos_done
```
]
.pull-right[

```
## # A tibble: 524 x 2
##    paragraph pos_done
##        &lt;int&gt; &lt;chr&gt;   
##  1         1 안녕    
##  2         1 대통령  
##  3         3 소말리아
##  4         3 해적    
##  5         3 소탕    
##  6         3 아덴만  
##  7         3 작전    
##  8         3 있다    
##  9         3 오늘    
## 10         3 북한    
## # ... with 514 more rows
```
]

---

## 함께 사용한 함수 설명

### `grepl()`

`grepl()` 함수는 글자 데이터내에 찾고자 하는 글자가 있는지를 T/F 결과로 돌려줌

```{}
grepl(
  pattern = 찾고자 하는 글자,  
  x = 글자 데이터,             
  fixed  = 정규식을 사용 여부, # T/F로 되어 있으며 FALSE가 기본값
  ...                     
  )
```

---

### `gsub()`

`gsub()` 함수는 찾고자 하는 글자를 원하는 글자로 바꿔줌

```{}
gsub(
  pattern = 찾고자 하는 글자,
  replacement = 찾은 글자가 바뀌게 될 글자,
  x = 글자 데이터,
  fixed  = 정규식을 사용 여부, # T/F로 되어 있으며 FALSE가 기본값
  ...                     
  )
```


### `nchar()`

`nchar()`는 글자 데이터를 받아서 글자수를 알려줌

```{}
nchar(
  x = 세고자 하는 글자,
  ...
  )
```

---

## 최소한의 정규 표현식

- `^` : 이걸로 시작함    
- `$` : 이걸로 끝남    
- `.` : 임의의 글자 하나    
- `?` : 앞에 있는 문자가 없거나 하나    
- `+` : 앞에 있는 문자가 하나 이상    
- `*` : 앞에 있는 문자가 없거나 하나 이상    

참고 : https://mrchypark.github.io/dabrp_classnote3/class4

---

## 연습문제

1. 아래 코드로 이명박 대통령 연설문 중 10개 를 가져오세요.
1. `grepl()` 함수를 사용해서 제목(title 컬럼)에 나눔이 있는 연설문을 찾으세요.
1. `gsub()` 함수를 사용해서 제목(title 컬럼)에 `인터넷 연설` 글자를 없애보세요.
1. `mutate()` 함수를 사용해서 연설문id를 1~10까지 id 컬럼으로 추가하세요.
1. `id`와 `content` 컬럼만 선택하세요.
1. 형태소 분석을 하여 결과를 pos 컬럼으로 추가하세요.
1. `grepl()` 함수를 사용해서 명사만 남겨보세요.
1. `gsub()` 함수를 사용해서 POS 정보를 지우고 한글만 남기세요.


```r
library(presidentSpeechKr)
library(tidyverse)
tar &lt;- 
  spidx %&gt;% 
  filter(president == "이명박") %&gt;% 
  arrange(date) %&gt;% 
  top_n(10) %&gt;% 
  mutate(spch = map(link, get_speech)) %&gt;% 
  select(spch) %&gt;% 
  unnest
tar
```




---

## 앞선 코드의 문제점

`활기찬다` 같은 글자가 발생


```r
pos_res %&gt;%
  filter(grepl("/va", pos)) %&gt;% 
  mutate(pos_done = gsub("/.*$", "다", pos))
```

```
## # A tibble: 30 x 4
##    paragraph pos           pos_order pos_done
##        &lt;int&gt; &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;   
##  1         5 좋/va                90 좋다    
##  2         5 좋/va               132 좋다    
##  3         5 활기찬/va+etm       176 활기찬다
##  4         7 큰/va+etm           200 큰다    
##  5         7 작/va               221 작다    
##  6         9 밝/va               288 밝다    
##  7         9 큰/va+etm           294 큰다    
##  8        13 높/va               338 높다    
##  9        15 높/va               432 높다    
## 10        17 좋/va               470 좋다    
## # ... with 20 more rows
```

---

## 문제를 해결해 보자

추가 패키지가 필요함


```r
library(tidyr)
library(purrr)
library(KoNLP)
```

---

### 문제를 해결해 보자

.pull-left[

```r
n_done &lt;- 
  pos_res %&gt;%
  filter(grepl("/n", pos)) %&gt;% 
  mutate(pos_done = gsub("/.*$", "", pos))

v_done &lt;- 
  pos_res %&gt;%
  filter(grepl("/v(a|v)$", pos)) %&gt;% 
  mutate(pos_done = gsub("/.*$", "다", pos))

sn_done &lt;- 
  pos_res %&gt;%
  filter(grepl("/sn", pos)) %&gt;% 
  mutate(pos_done = gsub("/.*$", "", pos))

jamos &lt;-
  function(text) {
    paste0(
      convertHangulStringToJamos(text)
      , collapse = "")
  }
```
]
.pull-right[

```r
etm_done &lt;-
  pos_res %&gt;%
  filter(grepl("/v(a|v)\\+etm", pos)) %&gt;%
  mutate(po = gsub("/.*$", "", pos)) %&gt;% 
  mutate(post = po %&gt;% 
           map(jamos)) %&gt;%
  unnest %&gt;%
  mutate(n = nchar(post)) %&gt;% 
  mutate(posts = substr(post, 1, n - 1)) %&gt;%
  mutate(done = posts %&gt;% 
           map_chr(HangulAutomata)) %&gt;% 
  mutate(pos_done = paste0(done, "다"))

pos_done&lt;- 
  bind_rows(sn_done,
          n_done,
          v_done,
          etm_done) %&gt;%
  filter(nchar(pos_done)&gt;1) %&gt;% 
  arrange(pos_order) %&gt;% 
  select(paragraph, pos_done)
```
]
---

## 단어 기반 지표

1. 단어 출현 빈도 : 단순히 단어가 나타난 횟수를 세서 확인
1. 동시 출현 빈도 : 기준 단어와 함께 나타난 단어들과 그 횟수를 세서 확인
1. tf-idf : 전체 문서에서 나타난 횟수와 개별 문서에서 나타난 횟수로 만든 지표
--
.center[
![](https://user-images.githubusercontent.com/6179259/47669547-78c9f180-dbee-11e8-85e8-e01cb4cbe46d.png)
]

---

### 단어 출현 빈도

`count()` 함수는 데이터에서 총 몇 번 나왔는지 세어주는 집계함수. `group_by()`와 함께 사용하여 각 연설문별 출현 횟수 등을 구할 수 있음.


```r
library(dplyr)
wn &lt;-
  pos_done %&gt;% 
  count(pos_done, sort=T)
wn
```

```
## # A tibble: 308 x 2
##    pos_done     n
##    &lt;chr&gt;    &lt;int&gt;
##  1 우리        26
##  2 국가        14
##  3 나라        11
##  4 세계        11
##  5 있다        11
##  6 국제         8
##  7 높다         8
##  8 브랜드       7
##  9 사회         7
## 10 되다         6
## # ... with 298 more rows
```

---

### 워드클라우드

`count()` 함수로 단어와 그 빈도 테이블을 만들었다면, {wordcloud} 패키지를 사용해서 워드클라우드를 만들 수 있음
.pull-left[
```r
library(wordcloud)
wn %&gt;% 
  with(wordcloud(pos_done, n))
```
]
.pull-right[


![](index_files/figure-html/unnamed-chunk-31-1.png)&lt;!-- --&gt;
]

---

### 빈도에 따른 색 입히기

&lt;https://github.com/EmilHvitfeldt/r-color-palettes&gt; 에 R에서 사용할 수 있는 색 테마 패키지들을 소개하고 있음.
.pull-left[
```r
## install.packages("Redmonder")
library(Redmonder)
pal = redmonder.pal(6, "sPBIRdPu")

wn %&gt;% 
  with(wordcloud(pos_done, 
                 n, 
                 colors = pal))
```
]
.pull-right[
![](index_files/figure-html/unnamed-chunk-32-1.png)&lt;!-- --&gt;
]

---

### 연습문제

* 49p 연습문제에서 사용한 `tar`를 이용해 진행해 주세요.    
* `tar` 는 이명박 대통령의 라디오 연설문 10건 입니다.

1. `tar`의 `content` 컬럼을 `pos()` 함수로 형태소 분석을 진행해주세요.
1. 그 중 명사만 남기고, 형태소 정보는 지워주세요. 한글자 명사도 지워주세요.
1. `count()` 함수를 이용해서 단어 출현 빈도를 계산해 주세요.
1. wordcloud를 만들어 주세요.
1. 다른 색 조합으로 시도해 주세요.
1. `group_by()`를 활용하여 각 연설문 별로 단어 출현 빈도를 계산해주세요.
1. 각 연설문에서 "우리"가 몇 번 사용되었는지 확인해주세요.

---

### 동시 출현 빈도

`pairwise_count()` 함수는 그룹 단위 내에서 단어가 동시에 출현한 횟수를 세어주는 함수. 보통 문장 단위를 그룹으로 처리

.pull-left[
```r
## install.packages("widyr" ,dependencies = T)
library(widyr)
pw_tar &lt;- 
  tar %&gt;% 
  unnest_tokens(sent, content, token="sentences") %&gt;% 
  mutate(id = as.numeric(1:n())) %&gt;% 
  unnest_tokens(pos, sent, token=pos) %&gt;% 
  select(id, pos) %&gt;% 
  filter(grepl("/n|/v(v|a)", pos)) %&gt;% 
  mutate(pos = gsub("/.*$","",pos)) %&gt;% 
  filter(nchar(pos) &gt; 1)

pw &lt;-
  pw_tar %&gt;% 
  pairwise_count(pos, id, sort = T, upper = F)
pw
```
]
.pull-right[

```
## # A tibble: 22,250 x 3
##    item1  item2      n
##    &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;
##  1 국민   여러분    51
##  2 우리   나라      24
##  3 우리   세계      23
##  4 국민   사랑      22
##  5 여러분 사랑      22
##  6 시장   전통      22
##  7 국민   우리      20
##  8 사회   우리      18
##  9 우리   경제      17
## 10 생각   우리      15
## # ... with 22,240 more rows
```
]

---

### 기준 단어로 데이터 탐색

`filter()` 함수로 기준 단어를 조회하면 함께 자주 나오는 단어와 그 빈도를 확인할 수 있음
.pull-left[

```r
pw %&gt;%
  filter(item1 == "우리")
```

```
## # A tibble: 713 x 3
##    item1 item2        n
##    &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;
##  1 우리  나라        24
##  2 우리  세계        23
##  3 우리  경제        17
##  4 우리  국가        12
##  5 우리  개발        12
##  6 우리  정부        11
##  7 우리  미래         9
##  8 우리  대한민국     9
##  9 우리  성장         9
## 10 우리  녹색         8
## # ... with 703 more rows
```
]
.pull-right[

```
## Selecting by n
```

![](index_files/figure-html/unnamed-chunk-35-1.png)&lt;!-- --&gt;

]
---

### 연습문제

* 49p 연습문제에서 사용한 `tar`를 이용해 진행해 주세요.    
* `tar` 는 이명박 대통령의 라디오 연설문 10건 입니다.

1. `tar`의 `content` 컬럼을 문장 단위로 나누어 주세요.
1. 새롭게 문장별 id를 `id` 컬럼으로 추가해주세요.
1. 문장별 id를 유지한 채로 `pos()` 함수를 사용하여 형태소 분석을 진행해 주세요.
1. 명사(`/n`), 동사(`/vv`), 형용사(`/va`)인 형태소만 가져와 주세요.
1. 형태소 정보는 제거하지 말고 그대로 두세요.
1. 동시 출현 빈도 테이블을 만들어 주세요. (컬럼이 item1, item2, n으로 구성됩니다.)
1. `우리/np`와 함께 출현한 단어들과 그 빈도를 확인하세요.
1. 명사는 형태소 정보를 제거하고, 형용사와 동사는 형태소 정보를 제거한후 뒤에 `다`를 붙여주세요.
1. 한 글자는 제거해 주세요.
1. 동시 출현 빈도 테이블을 만들어 주세요. (컬럼이 item1, item2, n으로 구성됩니다.)
1. `사랑`과 함께 출현한 단어들과 그 빈도를 확인하세요.
---

### 네트워크 시각화


```r
library(igraph)
pw_graph &lt;- 
  pw %&gt;% 
  filter(n &gt; 5) %&gt;%
  graph_from_data_frame()
pw_graph 
```

```
## IGRAPH 69baf5b DN-- 61 86 -- 
## + attr: name (v/c), n (e/n)
## + edges from 69baf5b (vertex names):
##  [1] 국민  -&gt;여러분   우리  -&gt;나라     우리  -&gt;세계     국민  -&gt;사랑    
##  [5] 여러분-&gt;사랑     시장  -&gt;전통     국민  -&gt;우리     사회  -&gt;우리    
##  [9] 우리  -&gt;경제     생각  -&gt;우리     온누리-&gt;상품권   사회  -&gt;기업    
## [13] 사회  -&gt;생각     기후  -&gt;기금     라디오-&gt;인터넷   라디오-&gt;연설    
## [17] 인터넷-&gt;연설     우리  -&gt;국가     성장  -&gt;녹색     녹색  -&gt;기후    
## [21] 녹색  -&gt;기금     우리  -&gt;개발     위기  -&gt;경제     우리  -&gt;정부    
## [25] 희망  -&gt;국민     안녕  -&gt;대통령   국민  -&gt;생각     시장  -&gt;상인    
## [29] 시장  -&gt;상품권   경제  -&gt;세계     경제  -&gt;글로벌   때문  -&gt;우리    
## + ... omitted several edges
```
---

### 네트워크 시각화 코드

네트워크 데이터는 node, edge로 구성됨

```r
library(ggraph)
set.seed(2018)

a &lt;- grid::arrow(type = "closed", length = unit(.1, "inches"))

ggraph(pw_graph) +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

---

### 네트워크 시각화
.center[

```
## Using `nicely` as default layout
```

![](index_files/figure-html/unnamed-chunk-37-1.png)&lt;!-- --&gt;
]
---

### tf-idf

* tf : 전체 문서내의 단어 빈도    
* idf : 단어를 가지는 문서 비율의 역수

.center[
![](https://user-images.githubusercontent.com/6179259/47669547-78c9f180-dbee-11e8-85e8-e01cb4cbe46d.png)
]

---

### tf-idf

`bind_tf_idf()` 함수가 `tf`, `idf`, `tf-idf` 점수 모두를 제공하며 문서 단위의 정의가 매우 중요함. 보통 각 연설문, 개별 뉴스 본문 등을 하나의 문서로 정의함. `tf-idf` 가 높을 수록 각 문서에서 특별한 의미를 지니는 것으로 판단할 수 있음. 


.pull-left[
```r
library(tidytext)

tfidf_tar &lt;- 
  tar %&gt;% 
  mutate(id = as.numeric(1:n())) %&gt;% 
  unnest_tokens(pos, content, token=pos) %&gt;% 
  select(id, pos) %&gt;% 
  filter(grepl("/n|/v(v|a)", pos)) %&gt;% 
  mutate(pos = gsub("/.*$","",pos)) %&gt;% 
  filter(nchar(pos) &gt; 1) %&gt;% 
  group_by(id) %&gt;% 
  count(pos)

tfidf_tar %&gt;% 
  bind_tf_idf(pos, id, n) %&gt;% 
  arrange(desc(tf_idf))
```
]
.pull-right[

```
## # A tibble: 3,428 x 6
## # Groups:   id [10]
##       id pos              n     tf   idf tf_idf
##    &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1     2 추석             7 0.0330  1.61 0.0531
##  2     8 선거             6 0.0208  2.30 0.0478
##  3     1 그린란드         9 0.0178  2.30 0.0409
##  4     6 아랍에미리트     9 0.0174  2.30 0.0401
##  5     4 기금            12 0.0232  1.61 0.0373
##  6     6 원전             8 0.0155  2.30 0.0356
##  7    10 브랜드           7 0.0155  2.30 0.0356
##  8     5 보험             8 0.0153  2.30 0.0352
##  9     1 북극             7 0.0138  2.30 0.0318
## 10     2 고향             4 0.0189  1.61 0.0304
## # ... with 3,418 more rows
```
]

---

### 연습문제

* 49p 연습문제에서 사용한 `tar`를 이용해 진행해 주세요.    
* `tar` 는 이명박 대통령의 라디오 연설문 10건 입니다.

1. 새롭게 연설문별 id를 `id` 컬럼으로 추가해주세요.
1. 문장별 id를 유지한 채로 `pos()` 함수를 사용하여 형태소 분석을 진행해 주세요.
1. 명사(`/n`), 동사(`/vv`), 형용사(`/va`)인 형태소만 가져와 주세요.
1. 명사는 형태소 정보를 제거하고, 형용사와 동사는 형태소 정보를 제거한후 뒤에 `다`를 붙여주세요.
1. 한 글자는 제거해 주세요.
1. 연설문 별로 형태소 단위 빈도를 계산해 주세요.
1. `bind_tf_idf()` 함수를 사용해서 `tf`, `idf`, `tf-idf` 를 계산해주세요.
1. 각 연설문 별로 `tf-idf` 점수가 가장 높은 단어를 확인하세요.
1. 각 연설문 별로 `tf-idf` 점수가 가장 높은 3개 단어씩을 확인하세요.

---

## 감성 분석

- 감성 분석은 각 단어의 감성 사전을 구축하여 점수를 주는 방식
- 한글의 특성상, 형태소이며 ngram에 점수를 부여하는 것이 가장 효과적일 것
- 단순한 형태로는 unigram의 형태소에 점수나 종류를 부여하는 것
- 개별 단어의 점수를 부여한 뒤 문장 단위로 합산하여 계산
- 합산으로 0에 가까운 값이 나올 수 도 있으므로 점수를 부여받은 단어의 갯수등 도 고려 필요
- 안정적으로 기구축된 한글 사전을 찾기 어려움

---

### 사전 소개

[KnuSentiLex](https://github.com/park1200656/KnuSentiLex)는 군산대 [Data Intelligence Lab](http://dilab.kunsan.ac.kr/)에서 기존 사전들을 참조, 활용하여 18년 구축한 감성 사전. 구조가 단순하고 이모티콘 등을 추가한 점이 장점인 반면, 형태소 형식이 아니라 점수의 신뢰도가 낮은 편임.

[KOSAC](http://word.snu.ac.kr/kosac/)은 서울대에서 13년에 구축한 감성사전으로 구조가 복잡하고 점수를 내기 어렵지만 12년에 구축한 감성 스키마를 따르고 있어 다양한 감성 정보를 얻을 수 있음.

본 예시에는 구조가 단순한 `KnuSentiLex`을 사용


```r
## remotes::install_github("mrchypark/KnuSentiLexR")
library(KnuSentiLexR)
senti_tar &lt;- 
  tar %&gt;% 
  unnest_tokens(sent, content, token="sentences") %&gt;% 
  filter(nchar(sent) &lt; 20) %&gt;% 
  select(sent)
```

---

### 감성 분석

- `senti_score()` 함수는 문장을 unigram 부터 3-gram 까지 작성한 후, 감성 사전에 점수를 합산하여 문장 점수를 계산    
- `senti_magnitude()` 함수는 몇개의 ngram이 점수화되었는지를 계산    
- `dic` 객체가 word, polarity 컬럼을 가지고 있는 감성 사전임


```r
senti_tar %&gt;% 
  mutate(score = senti_score(sent),
         magni = senti_magnitude(sent)) %&gt;% 
  filter(score != 0)
```

```
## # A tibble: 38 x 3
##    sent                              score magni
##    &lt;chr&gt;                             &lt;dbl&gt; &lt;dbl&gt;
##  1 먼저 함께 보시죠.                     1     1
##  2 자랑 좀 해 보세요.                   -2     1
##  3 사회자 눈물이 그렁그렁하네요.        -1     1
##  4 이상하게.                            -1     1
##  5 이분은 ‘한번 해 보자.’               -2     1
##  6 대통령 그렇게 해 주세요.             -2     1
##  7 “난 부모 잘 만났어요.                 1     1
##  8 갔더니 정말 친절하게 잘해 줬어요.     4     2
##  9 그런데 설명을 다 해 주시더라고요.    -2     1
## 10 그거 대단한 거죠.                     2     1
## # ... with 28 more rows
```

---
class: title, center, middle

## 파일 불러오기

---

### 폴더 내의 파일들 한 번에 불러오기

```
library(fs)
library(tidyverse)
"data_dir" %&gt;% 
  fs::dir_ls(regexp = "\\.csv$") %&gt;% 
  purrr::map_dfr(readr::read_csv, .id = "source") %&gt;% 
  dplyr::mutate(Month_Year = lubridate::myd(Month_Year, truncated = 1))
```

[출처](https://mrchypark.github.io/post/%EB%B2%88%EC%97%AD-%ED%8F%B4%EB%8D%94%EC%95%88%EC%9D%98-csv-%ED%8C%8C%EC%9D%BC%EB%93%A4%EC%9D%84-purrr-%EC%99%80-readr-%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%B4%EC%84%9C-%ED%95%9C%EB%B0%A9%EC%97%90-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0/)

---
class: title, center, middle

## 예시 프로젝트

---





[1]: https://github.com/tidyverse/tidyverse
[2]: https://mrchypark.github.io/kisa_finR
[tidytextmining]: https://www.tidytextmining.com/
[tidytext]: https://juliasilge.github.io/tidytext/
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
